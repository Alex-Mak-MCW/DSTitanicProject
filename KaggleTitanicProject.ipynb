{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-28T02:35:24.574439Z","iopub.status.busy":"2023-06-28T02:35:24.574027Z","iopub.status.idle":"2023-06-28T02:35:24.582535Z","shell.execute_reply":"2023-06-28T02:35:24.581387Z","shell.execute_reply.started":"2023-06-28T02:35:24.574412Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","# Import modules for future uses\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# Read files (in kaggle way)\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T02:35:24.624504Z","iopub.status.busy":"2023-06-28T02:35:24.623264Z","iopub.status.idle":"2023-06-28T02:35:24.647160Z","shell.execute_reply":"2023-06-28T02:35:24.645820Z","shell.execute_reply.started":"2023-06-28T02:35:24.624458Z"},"trusted":true},"outputs":[],"source":["# read train.csv as training data in the format as data frame\n","train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n","# use head() to get the top of the data\n","train_data.head()\n","# print(train_data.shape) # 891 "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T02:35:24.668083Z","iopub.status.busy":"2023-06-28T02:35:24.667705Z","iopub.status.idle":"2023-06-28T02:35:24.687008Z","shell.execute_reply":"2023-06-28T02:35:24.685845Z","shell.execute_reply.started":"2023-06-28T02:35:24.668056Z"},"trusted":true},"outputs":[],"source":["# read test.csv as testing data in the format of pd data frame\n","test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n","# use head() to get the top of the data\n","test_data.head()\n","# print(test_data.shape) # 418"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check data to find any data missing (NaN)\n","\n","# Use df.describe to show descriptive stat for NUMERICAL VAR ONLY\n","# use include=all to show CATEGORICAL VAR too\n","train_data.describe(include='all')\n","# Found NaNs in Age, Name, Ticket, Cabin, Embarked"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Handle missing/ duplicate data\n","\n","# Count all null values using df.isnull().sum()\n","print('\\nNull values in training data: \\n{0}'.format(train_data.isnull().sum()))\n","# Found Null values in Age(177), Cabin (687) and Embarked (2)\n","print('\\nNull values in testing data:  \\n{0}'.format(test_data.isnull().sum()))\n","# Found Null values in Age(86), Fare(1), Cabin (327)\n","\n","# Count all duplicated values using df.duplicated().sum()\n","print('\\nDuplicated Values in training data {0}'.format(train_data.duplicated().sum()))\n","print('Duplicated values in testing data {0}'.format(test_data.duplicated().sum()))\n","# Found No duplicate data below"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Filling Data to make it complete\n","\n","# 1. Fill Embarked in training by using the most common port of embarkment ()\n","# Find the most common port in training data\n","# Use the df[column name].value_counts() to show the counts of each value in col\n","# Then use .idxmax() to return the mode (most common) value\n","mode=train_data[\"Embarked\"].value_counts().idxmax()\n","\n","# Replace null with the mode value using df[Column name].fillna\n","# MAKE SURE use the argument inplace=True to make sure the replacement is done on current dataFrame\n","# Otherwise it will make a copy of dataframe (which you can assign it)\n","train_data[\"Embarked\"].fillna(mode, inplace=True)\n","\n","# check whether null values exist (for testing)\n","# print(train_data[\"Embarked\"].isnull().sum()==0)\n","\n","# 2. Fill Fare with the average value of all fares\n","# Find the average fare value in testing data\n","# Use the df[column name].mean to show the mean of all values in a col\n","average=test_data[\"Fare\"].mean()\n","\n","# Just like above\n","test_data[\"Fare\"].fillna(average,inplace=True)\n","\n","# checking incase\n","# print(test_data[\"Fare\"].isnull().sum()==0)\n","\n","# 3. Fill Age\n","# First find the average age for each name title, then put mean age back to missing data based on its name title\n","\n","# 3.1 Find mean age for each name title\n","# find values in column A based on a conidition in column B:\n","# df[conidition on B][column A]\n","# use mean() to find average and round() to make sure it's whole #\n","\n","mean_age_mr=train_data[train_data[\"Name\"].str.contains('Mr.', na=False)][\"Age\"].mean().round()\n","mean_age_mrs=train_data[train_data[\"Name\"].str.contains('Mrs.', na=False)][\"Age\"].mean().round()\n","mean_age_miss=train_data[train_data[\"Name\"].str.contains('Miss.', na=False)][\"Age\"].mean().round()\n","mean_age_master=train_data[train_data[\"Name\"].str.contains('Master.', na=False)][\"Age\"].mean().round()\n","mean_age_dr=train_data[train_data[\"Name\"].str.contains('Dr.', na=False)][\"Age\"].mean().round()\n","\n","# Rest filled with non NaN values\n","mean_age_rev=train_data[train_data[\"Name\"].str.contains('Rev.', na=False)][\"Age\"].mean().round()\n","mean_age_mme=train_data[train_data[\"Name\"].str.contains('Mme.', na=False)][\"Age\"].mean().round()\n","mean_age_major=train_data[train_data[\"Name\"].str.contains('Major.', na=False)][\"Age\"].mean().round()\n","mean_age_lady=train_data[train_data[\"Name\"].str.contains('Lady.', na=False)][\"Age\"].mean().round()\n","mean_age_sir=train_data[train_data[\"Name\"].str.contains('Sir.', na=False)][\"Age\"].mean().round()\n","mean_age_col=train_data[train_data[\"Name\"].str.contains('Col.', na=False)][\"Age\"].mean().round()\n","mean_age_mlle=train_data[train_data[\"Name\"].str.contains('Mlle.', na=False)][\"Age\"].mean().round()\n","mean_age_capt=train_data[train_data[\"Name\"].str.contains('Capt.', na=False)][\"Age\"].mean().round()\n","mean_age_countess=train_data[train_data[\"Name\"].str.contains('Countess.', na=False)][\"Age\"].mean().round()\n","\n","# replace empty age with the mean age based on name title\n","\n","# print(train_data[\"Age\"].isnull().sum()) # 177\n","\n","# function to replace values from NaN with means\n","title_list=['Mrs.','Miss.', 'Mr.', 'Master.', 'Dr.']\n","mean_list=[mean_age_mrs, mean_age_miss, mean_age_mr, mean_age_master, mean_age_dr]\n","\n","for i in range(len(title_list)):\n","    train_data.loc[train_data[\"Name\"].str.contains(title_list[i], na=False) & train_data[\"Age\"].isna(),\"Age\"]=mean_list[i]\n","\n","# print(train_data[\"Age\"].isnull().sum()) # 0\n","\n","# 4. Actually Fill Cabin\n","\n","# Fill cabin by check their fare to guess their cabin\n","\n","# print mean fare for each cabin\n","\n","# mean_fare_cabin_a=train_data[train_data[\"Cabin\"]=='A'][\"Fare\"].mean() # left wasted due to no correlation\n","mean_fare_cabin_b=train_data[train_data[\"Cabin\"]=='B'][\"Fare\"].mean()\n","mean_fare_cabin_c=train_data[train_data[\"Cabin\"]=='C'][\"Fare\"].mean()\n","mean_fare_cabin_d=train_data[train_data[\"Cabin\"]=='D'][\"Fare\"].mean()\n","mean_fare_cabin_e=train_data[train_data[\"Cabin\"]=='E'][\"Fare\"].mean()\n","mean_fare_cabin_f=train_data[train_data[\"Cabin\"]=='F'][\"Fare\"].mean()\n","mean_fare_cabin_g=train_data[train_data[\"Cabin\"]=='G'][\"Fare\"].mean()\n","\n","# assign cabin base on fares, replace all unknown fare \n","\n","# print(train_data.loc[(train_data[\"Cabin\"]=='X')][\"Cabin\"].value_counts().to_dict())\n","train_data.loc[(train_data[\"Cabin\"]=='X')&(train_data[\"Fare\"]>=mean_fare_cabin_b), \"Cabin\"]=\"B\"\n","train_data.loc[(train_data[\"Cabin\"]=='X')&(train_data[\"Fare\"]<mean_fare_cabin_b) & (train_data[\"Fare\"]>=mean_fare_cabin_c), \"Cabin\"]=\"C\"\n","train_data.loc[(train_data[\"Cabin\"]=='X')&(train_data[\"Fare\"]<mean_fare_cabin_c) & (train_data[\"Fare\"]>=mean_fare_cabin_d), \"Cabin\"]=\"D\"\n","train_data.loc[(train_data[\"Cabin\"]=='X')&(train_data[\"Fare\"]<mean_fare_cabin_d) & (train_data[\"Fare\"]>=mean_fare_cabin_e), \"Cabin\"]=\"E\"\n","train_data.loc[(train_data[\"Cabin\"]=='X')&(train_data[\"Fare\"]<mean_fare_cabin_e) & (train_data[\"Fare\"]>=mean_fare_cabin_f), \"Cabin\"]=\"F\"\n","train_data.loc[(train_data[\"Cabin\"]=='X')&(train_data[\"Fare\"]<mean_fare_cabin_f) & (train_data[\"Fare\"]>=mean_fare_cabin_g), \"Cabin\"]=\"G\"\n","\n","# print(train_data[(train_data[\"Cabin\"]=='X')&train_data[\"Fare\"]>mean_fare_cabin_g]) # Empty data frame check to ensure all fare above the mean of cabin G has assigned to a new cabin\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# How to Fill Cabin (supplementry): \n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# a. first group the unknown data into a group\n","# for known data, keep the first character\n","# Then display using seaborn's boxplot\n","\n","# Training and testing data\n","train_data[\"Cabin\"]=pd.Series('X'if pd.isnull(each) else each[0] for each in train_data[\"Cabin\"])\n","test_data[\"Cabin\"]=pd.Series('X'if pd.isnull(each) else each[0] for each in test_data[\"Cabin\"])\n","\n","# Plot Cabin data with fare to check how the unknown usually belong\n","plt.figure(figsize=(12,5))\n","plt.title('Box Plot of Temperatures by Modules')\n","sns.boxplot(x='Cabin',y='Fare',data=train_data, palette='Set2')\n","plt.tight_layout()\n","\n","# Result shows most from the unknown class (X) has a low fare\n","        \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check entire dataset to make sure to null value\n","\n","train_data.isnull().sum()\n","# Return 0 for every column"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Feature Engineering: \n","# * analyze features and extract impactful info from it\n","# * Even create new features from existing one\n","\n","# 1. Analyze through graphs (Visualization) through sns and plt\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","# Create a figure and a set of subplots using plt.subplots\n","# plt.subplots(row, column, figsize(x,y))\n","fig, axx=plt.subplots(1,3, figsize=(20,5))\n","\n","# Using sns.histplot (histogram) or countplot (to show different colors)\n","# Make a graph SibSp X count with title\n","sns.countplot(data=train_data, x=\"SibSp\", ax=axx[0]).set(title=\"Number of Sibilings/ Spouses\")\n","\n","# Make a graph Parch X count with title\n","sns.countplot(data=train_data, x=\"Parch\", ax=axx[1]).set(title=\"Number of Parents/ Child\")\n","\n","# Make a graph Pclass X count with title\n","sns.countplot(data=train_data, x=\"Pclass\", ax=axx[2]).set(title=\"Distribution of Classes\")\n","\n","# use plt.tight_layout to adjust padding among subplots\n","plt.tight_layout()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Insights learned from the plots:\n","1. Most people are in 3rd class\n","2. Most people don't have parents/children \n","3. Most people don't have sibilings/spouses\n","\n","# Conclusions made\n","1. can create a feature to determine whether the passenger is alone or not (with family)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create alone feature for both training and testing data\n","\n","# For training data\n","# Way 1. By creating a function and use .apply to apply \n","# def create_alone_feature(SibSp_Parch):\n","#     if (SibSp_Parch[0]+SibSp_Parch[1])==0:\n","#         return 1\n","#     else:\n","#         return 0\n","# train_data['Alone'] = train_data[['SibSp','Parch']].apply(create_alone_feature, axis=1)\n"," \n","# way 2: use np.where\n","# np.where(condition, value if condition is true, value if condition is false)\n","train_data['Alone']=np.where((train_data['SibSp']+train_data['Parch']==0),1, 0)\n","# FamilySize: 1 + # of SibSp + # of ParCh\n","# make new column based on arithmetic, vector addition\n","train_data['FamilySize'] = 1 + train_data['SibSp'] + train_data['Parch']\n","\n","# verified using df.equals\n","# print(train_data['Alone'].equals(train_data['Alone2']))\n","\n","# For testing data\n","test_data['Alone']=np.where((test_data['SibSp']+test_data['Parch']==0),1, 0)\n","# FamilySize: 1 + # of SibSp + # of ParCh\n","test_data['FamilySize'] = 1 + test_data['SibSp'] + test_data['Parch']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# More Visualization for feature engineering\n","\n","# make subplots in 2x3 size, set figure size\n","fig, axx = plt.subplots(2, 3, figsize=(20,10))\n","sns.countplot(x='Survived', data=train_data, ax=axx[0,0]).set(title='Survivors')\n","sns.countplot(x='Survived', hue='Sex', data=train_data, ax=axx[0,1]).set(title='Survivors by Sex')\n","sns.countplot(x='Survived', hue='Pclass', data=train_data, ax=axx[0,2]).set(title='Survivors by Pclass')\n","sns.countplot(x='Survived', hue='Alone', data=train_data, ax=axx[1,0]).set(title='Accompanied survivors')\n","sns.countplot(x='FamilySize', hue='Survived', data=train_data, ax=axx[1,1]).set(title='Accompanied survivors')\n","sns.countplot(x='Pclass', hue='Alone', data=train_data, ax=axx[1,2]).set(title='Alone members by Pclass')\n","plt.tight_layout()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Insights from the plots above:\n","\n","## First plot:\n","1. More deceased than survived (50-60%)\n","\n","## Second plot:\n","1. Male passengers more likely to die than survive\n","2. Female passengers more likely to survive than die\n","\n","## Third plot:\n","1. Third passenger class way more likely to die than first and second\n","\n","## Fourth plot:\n","1. Passengers who are alone are more likely to die\n","\n","## Fifth plot:\n","1. likelihood to decease much higher when alone\n","2. likelihood to survive much higher when not alone\n","\n","## Sixth plot\n","1. passengers in the third class are more likely to be alone"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T02:35:24.689120Z","iopub.status.busy":"2023-06-28T02:35:24.688710Z","iopub.status.idle":"2023-06-28T02:35:24.696447Z","shell.execute_reply":"2023-06-28T02:35:24.695312Z","shell.execute_reply.started":"2023-06-28T02:35:24.689093Z"},"trusted":true},"outputs":[],"source":["# pd.loc to access data in array based on given column(s)\n","# Find female survivors in training data\n","women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\n","# Find the rate of survived women by adding the (0/1) then divide the amount of women\n","rate_women = sum(women)/len(women)\n","\n","# Print the percentage of women\n","print(\"% of women who survived:\", rate_women)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T02:35:24.698873Z","iopub.status.busy":"2023-06-28T02:35:24.697850Z","iopub.status.idle":"2023-06-28T02:35:24.713740Z","shell.execute_reply":"2023-06-28T02:35:24.712237Z","shell.execute_reply.started":"2023-06-28T02:35:24.698834Z"},"trusted":true},"outputs":[],"source":["# pd.loc to access data in array based on given column(s)\n","# Find male survivors in training data\n","men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\n","# Find the rate of survived women by adding the (0/1) then divide the amount of women\n","rate_men = sum(men)/len(men)\n","\n","# Print the percentage of women\n","print(\"% of men who survived:\", rate_men)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Random Forest Classifier Parameters:\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n","\n","Random ForestTree parameter (learned from sklearn):\n","\n","n_estimators: number of trees in a forest\n","\n","criterion: Function to measure split {“gini”, “entropy”, “log_loss”}\n","\n","max_depth: tree max depth\n","\n","min_samples_split: min # of samples to split, usually 2\n","\n","min_samples_leaf: min # of samples to be a leaf, usually 1\n","\n","min_weight_fraction_leaf: min weight fraction of the sum of all weights to be a leaf node, usually 0.0\n","\n","max_feature: # of features to consider when look at best split, usually int, RandomState instance or None, default=None\n","\n","max_leaf_nodes: usually int or default None aka unlimited\n","\n","min_impurity_decrease: node splits if this split induce an impunity larger than given value\n","\n","bootstrap: whether bootstrap samples used in tree (Boolean), defualt True\n","\n","oob_score: whether use out of bag(oob) samples to estiamte generalization score (Boolearn, defualt=False)\n","\n","n_jobs: # of jobs run in parallel\n","\n","random_state: randomness of the estimator, usually nt, RandomState instance or None, default=None, used because when max_features<=nfeatures algo always select max_features\n","\n","verbose: controls the verbosity when fitting and predicting\n","\n","warm_start: when set True, resue solution from last call to fit and add more estimator, ELSE start a new tree\n","\n","class_weight: weighting of classes, usually in dict, list of dict or “balanced”, default=None\n","\n","ccp_alpha: complexity parameter used for cost-complexity pruning (CCP), default=0.0\n","\n","max_samples: if bootstrap set True, the # of samples to draw from X to train each base estimator"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T02:35:24.716019Z","iopub.status.busy":"2023-06-28T02:35:24.715603Z","iopub.status.idle":"2023-06-28T02:35:24.951780Z","shell.execute_reply":"2023-06-28T02:35:24.950668Z","shell.execute_reply.started":"2023-06-28T02:35:24.715983Z"},"trusted":true},"outputs":[],"source":["# # Implementing Random Forest Model\n","# from sklearn.ensemble import RandomForestClassifier\n","# from sklearn.metrics import accuracy_score\n","# # assign y to the survived passengers in training data\n","# y = train_data[\"Survived\"]\n","\n","# # Build trees based on the features selected\n","# # features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]\n","# features = [\"Sex\", \"Fare\"]\n","# # Assign X/X_test to the numerical variables from the training/testing data\n","# # Use pd.get_dummies to convert categorical variable to a numerical one\n","# X = pd.get_dummies(train_data[features])\n","\n","# X_test = pd.get_dummies(test_data[features])\n","# X_test['Fare'] = X_test['Fare'].fillna(0)\n","\n","\n","# # Implement random forest classifier with argument\n","# # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n","# # Original model: 0.785\n","# # model = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=1) \n","\n","# # Hyperparamter Tuning: \n","# # Optimized Model 1: gini criterion, 1000 DEPTH, rest untouched: ~0.76-0.77\n","# # Optimized Model 2: gini criterion, 10000 DEPTH, rest untouched: ~0.77\n","# # Optimized Model 3: gini criterion, 10 DEPTH, n_estimators=10000, rest untouched: ~0.78\n","# # Optimized Model 4: gini criterion, 1000 DEPTH, n_estimators=10000, rest untouched: ~0.78\n","# # Optimized Model 5: gini criterion, n_estimators=1000000, rest untouched: ~0.78\n","# # Optimized Model 6: gini criterion, n_estimators=1000, rest untouched: ~0.76, class_weight adjusted in which survivor is weighted heavier\n","# # Optimized Model 7: gini criterion, n_estimators=1000, rest untouched: ~0.76, class_weight adjusted in balance\n","# # Optimized Model 8: featues become only Sex and Fare, rest untouched: ~0.76\n","\n","\n","# # dict_weights = {1:2, 0: 1, 2:1, 3:1, 4:1}\n","# # dict_weights = {\"Pclass\":1, \"Sex\": 1, \"SibSp\": 1, \"Parch\": 1, \"Fare\": 1}\n","\n","# optimized_model = RandomForestClassifier(n_estimators=10000, criterion= \"gini\", max_features=None, bootstrap=True)\n","\n","# # TRY to fit the model\n","# optimized_model.fit(X, y)\n","# # use model.predict to save the model's predictions\n","# predictions = optimized_model.predict(X_test)\n","\n","# # Assign output (418x2) to a dataframe\n","# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n","# # Assign output to a csv file\n","# output.to_csv('submission.csv', index=False)\n","# print(\"Your submission was successfully saved!\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Decision Tree parameter (learned from sklearn):\n","\n","criterion: Function to measure split {“gini”, “entropy”, “log_loss”}\n","\n","splitter: strat to split each node, best for best split; random for best random split {“best”, “random”}\n","\n","max_depth: tree max dept, usually none\n","\n","min_samples_split: min # of samples to split, usually 1\n","\n","min_samples_leaf: min # of samples to be a leaf, usually 1\n","\n","min_weight_fraction_leaf: min weight fraction of the sum of all weights to be a leaf node\n","\n","max_feature: # of features to consider when look at best split, usually int, RandomState instance or None, default=None\n","\n","random_state: randomness of the estimator, usually nt, RandomState instance or None, default=None, used because when max_features<=nfeatures algo always select max_features\n","\n","max_leaf_nodes: usually int or default None aka unlimited\n","\n","min_impurity_decrease: node splits if this split induce an impunity larger than given value\n","\n","class_weight: weighting of classes, usually in dict, list of dict or “balanced”, default=None\n","\n","ccp_alpha: complexity parameter used for cost-complexity pruning (CCP), default=0.0"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
